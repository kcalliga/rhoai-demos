{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd, yaml\n",
    "from pathlib import Path\n",
    "from utils.graph import build_from_snapshot, Graph\n",
    "from utils.episodes import build_episodes, load_rules, apply_rules, episode_to_incident\n",
    "\n",
    "DATA_DIR = Path(os.environ.get(\"DATA_DIR\", \"data\"))\n",
    "OUT_DIR  = Path(os.environ.get(\"OUT_DIR\", \"incidents\"))\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RULES_PATH = \"rules/rules.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804316cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect a parquet or CSV that already merges audit/app/infra with a common schema.\n",
    "# Required columns: ts, source, namespace, pod, node, level, code, verb, msg, (optionally) rollout_in_window, container_restart\n",
    "df = pd.read_parquet(DATA_DIR / \"unified_logs/latest.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = json.loads((DATA_DIR / \"topology_snapshot.json\").read_text())\n",
    "graph: Graph = build_from_snapshot(snapshot)\n",
    "print(f\"Graph nodes={len(graph.meta)} edges={sum(len(v) for v in graph.adj.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = build_episodes(df, window=\"10min\", keys=[\"namespace\",\"pod\",\"node\"])\n",
    "len(episodes), episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = load_rules(RULES_PATH)\n",
    "written = 0\n",
    "for ep in episodes:\n",
    "    cands = apply_rules(ep, rules, graph)\n",
    "    incident = episode_to_incident(ep, cands)\n",
    "    out = OUT_DIR / f\"{ep.episode_id}.json\"\n",
    "    out.write_text(json.dumps(incident, indent=2))\n",
    "    written += 1\n",
    "written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cac43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = []\n",
    "for f in OUT_DIR.glob(\"*.json\"):\n",
    "    inc = json.loads(f.read_text())\n",
    "    top = inc[\"candidates\"][0] if inc[\"candidates\"] else None\n",
    "    tbl.append({\n",
    "        \"episode_id\": inc[\"episode_id\"],\n",
    "        \"start\": inc[\"start\"],\n",
    "        \"end\": inc[\"end\"],\n",
    "        \"count\": inc[\"features\"].get(\"count\", 0),\n",
    "        \"error_ratio\": inc[\"features\"].get(\"error_ratio\", 0.0),\n",
    "        \"top_component\": top[\"component\"] if top else None,\n",
    "        \"top_reason\": top[\"reason\"] if top else None,\n",
    "        \"top_score\": top[\"score\"] if top else 0.0,\n",
    "    })\n",
    "pd.DataFrame(tbl).to_parquet(OUT_DIR / \"incidents_index.parquet\", index=False)\n",
    "pd.DataFrame(tbl).head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
