{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”Ž Loki (multi-tenant) â†’ Parquet â†’ Episodes\n",
        "\n",
        "This notebook pulls logs from **application**, **infrastructure**, and **audit** tenants on an OpenShift Loki gateway,\n",
        "normalizes them with a JSON-first projector, writes `data/unified_logs/latest.parquet` & CSV, and builds 10â€‘minute episodes.\n",
        "It includes robust auth/SSL handling, nanosecond timestamp parsing, and tenant-specific helpers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed, install deps (uncomment once)\n",
        "# %pip install --quiet pandas numpy requests pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Window: 2025-09-10 17:40:09.196834+00:00 â†’ 2025-09-10 19:10:09.196834+00:00\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os, pandas as pd, numpy as np\n",
        "\n",
        "# --- Storage locations\n",
        "DATA_DIR = Path(\"data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "UNIFIED_DIR = DATA_DIR / \"unified_logs\"; UNIFIED_DIR.mkdir(exist_ok=True, parents=True)\n",
        "INCIDENTS_DIR = Path(\"incidents\"); INCIDENTS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "RULES_DIR = Path(\"rules\"); RULES_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# --- Time window (adjust as needed)\n",
        "END   = pd.Timestamp.utcnow()\n",
        "START = END - pd.Timedelta(\"90min\")\n",
        "print(\"Window:\", START, \"â†’\", END)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Loki helpers (tenantâ€‘aware + token + SSL toggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1.1 â€” config + session + diagnostics\n",
        "import pandas as pd, requests, urllib3\n",
        "\n",
        "# ---- Config (set env vars or edit here) ----\n",
        "LOKI_BASE       = os.environ.get(\"LOKI_BASE\", \"https://logging-loki-openshift-logging.apps.rhoai.ocp-poc-demo.com\")\n",
        "LOKI_TOKEN      = os.environ.get(\"LOKI_TOKEN\", \"<REDACTED>\")                   # e.g. export LOKI_TOKEN=\"$(oc whoami -t)\"\n",
        "LOKI_INSECURE   = os.environ.get(\"LOKI_INSECURE\", \"true\").lower() in (\"1\",\"true\",\"yes\")\n",
        "LOKI_ORG_ID     = os.environ.get(\"LOKI_ORG_ID\")                  # some gateways require X-Scope-OrgID\n",
        "LOKI_BASIC_USER = os.environ.get(\"LOKI_BASIC_USER\")\n",
        "LOKI_BASIC_PASS = os.environ.get(\"LOKI_BASIC_PASS\")\n",
        "\n",
        "if LOKI_INSECURE:\n",
        "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "_session = requests.Session()\n",
        "_default_headers = {\"Accept\": \"application/json\"}\n",
        "if LOKI_TOKEN:\n",
        "    _default_headers[\"Authorization\"] = f\"Bearer {LOKI_TOKEN}\"\n",
        "if LOKI_ORG_ID:\n",
        "    _default_headers[\"X-Scope-OrgID\"] = LOKI_ORG_ID\n",
        "\n",
        "def _debug_response(resp):\n",
        "    ct = resp.headers.get(\"Content-Type\", \"\")\n",
        "    preview = (resp.text or \"\")[:500]\n",
        "    return f\"HTTP {resp.status_code} CT={ct} URL={resp.url}\\nBody (first 500):\\n{preview}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tenant=application OK, ~10 labels\n",
            "tenant=infrastructure OK, ~11 labels\n",
            "tenant=audit OK, ~4 labels\n"
          ]
        }
      ],
      "source": [
        "# Cell 1.2 â€” tenant ping / labels / query_range (nanosecond safe)\n",
        "def loki_ping_tenant(tenant: str):\n",
        "    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/labels\"\n",
        "    r = _session.get(url, headers=_default_headers, timeout=30, verify=not LOKI_INSECURE, allow_redirects=False)\n",
        "    if not r.ok or \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
        "        raise RuntimeError(f\"Ping failed for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "    return r.json()\n",
        "\n",
        "def loki_labels_tenant(tenant: str):\n",
        "    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/labels\"\n",
        "    r = _session.get(url, headers=_default_headers, timeout=30, verify=not LOKI_INSECURE, allow_redirects=False)\n",
        "    r.raise_for_status()\n",
        "    return r.json().get(\"data\", [])\n",
        "\n",
        "def loki_query_range_tenant(tenant: str, expr, start_ts, end_ts, step='15s', limit=5000, direction='forward'):\n",
        "    \"\"\"\n",
        "    Robust query_range that ALWAYS returns a pandas DataFrame (possibly empty).\n",
        "    It handles redirects, non-JSON, odd timestamp formats, and unexpected payloads.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/query_range\"\n",
        "    params = {\n",
        "        \"query\": expr,\n",
        "        \"start\": int(pd.Timestamp(start_ts).value),  # ns\n",
        "        \"end\": int(pd.Timestamp(end_ts).value),\n",
        "        \"step\": step,\n",
        "        \"limit\": str(limit),\n",
        "        \"direction\": direction,\n",
        "    }\n",
        "    auth = (LOKI_BASIC_USER, LOKI_BASIC_PASS) if (LOKI_BASIC_USER and LOKI_BASIC_PASS) else None\n",
        "    r = _session.get(url, params=params, headers=_default_headers, timeout=60,\n",
        "                     verify=not LOKI_INSECURE, auth=auth, allow_redirects=False)\n",
        "\n",
        "    # Redirects usually mean OAuth login page\n",
        "    if r.is_redirect or r.status_code in (301,302,303,307,308):\n",
        "        raise RuntimeError(f\"Auth redirect for tenant={tenant}.\\n\" + _debug_response(r))\n",
        "\n",
        "    # Must be OK and JSON\n",
        "    if not r.ok:\n",
        "        raise RuntimeError(f\"Loki query_range failed for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "    if \"application/json\" not in (r.headers.get(\"Content-Type\", \"\")).lower():\n",
        "        raise RuntimeError(f\"Non-JSON response for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "\n",
        "    # Parse JSON safely\n",
        "    try:\n",
        "        payload = r.json()\n",
        "    except Exception:\n",
        "        # Return empty DF but surface the raw body in an error to the caller\n",
        "        raise RuntimeError(f\"Could not parse JSON for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "\n",
        "    # Defensive: tolerate odd shapes\n",
        "    data = {}\n",
        "    if isinstance(payload, dict):\n",
        "        data = payload.get(\"data\") or {}\n",
        "    results = data.get(\"result\") if isinstance(data, dict) else None\n",
        "    if results is None:\n",
        "        # Return empty DF rather than None\n",
        "        return pd.DataFrame(columns=[\"ts\", \"line\"])\n",
        "\n",
        "    def _parse_ns(ts_val):\n",
        "        s = str(ts_val)\n",
        "        try:\n",
        "            ns = int(s)\n",
        "        except ValueError:\n",
        "            ns = int(float(s))  # handles \"1.757e+18\"\n",
        "        return pd.to_datetime(ns, unit=\"ns\", utc=True)\n",
        "\n",
        "    rows = []\n",
        "    for series in results or []:\n",
        "        labels = series.get(\"metric\", {})\n",
        "        # Loki can return either \"values\" (range vector) or \"value\" (instant)\n",
        "        values = series.get(\"values\") or []\n",
        "        if not values and \"value\" in series:\n",
        "            values = [series[\"value\"]]\n",
        "        for ts, line in values:\n",
        "            rows.append({\"ts\": _parse_ns(ts), \"line\": line, **labels})\n",
        "\n",
        "    return pd.DataFrame(rows, columns=[\"ts\",\"line\", *({} if not rows else rows[0].keys())])\n",
        "\n",
        "def tenant_wildcard_selector(tenant: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns a selector like {k8s_namespace_name=~\".+\"} using a label that exists in the tenant.\n",
        "    Loki requires at least one matcher that cannot match empty.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        labels = set(loki_labels_tenant(tenant) or [])\n",
        "    except Exception:\n",
        "        labels = set()\n",
        "\n",
        "    # Try likely k8s labels first, then fall back.\n",
        "    prefs = [\n",
        "        \"k8s_namespace_name\", \"kubernetes_namespace_name\",\n",
        "        \"k8s_pod_name\", \"kubernetes_pod_name\",\n",
        "        \"k8s_node_name\", \"kubernetes_host\",\n",
        "        \"log_type\", \"namespace\", \"pod\", \"node\",\n",
        "        \"job\", \"filename\"\n",
        "    ]\n",
        "    for key in prefs:\n",
        "        if key in labels:\n",
        "            return f'{{{key}=~\".+\"}}'  # non-empty regex satisfies Loki\n",
        "    # absolute fallback: use whatever first label exists\n",
        "    if labels:\n",
        "        key = sorted(labels)[0]\n",
        "        return f'{{{key}=~\".+\"}}'\n",
        "    # last resort (shouldnâ€™t happen if tenant has data)\n",
        "    return '{job=~\".+\"}'\n",
        "\n",
        "    auth = (LOKI_BASIC_USER, LOKI_BASIC_PASS) if (LOKI_BASIC_USER and LOKI_BASIC_PASS) else None\n",
        "    r = _session.get(url, params=params, headers=_default_headers, timeout=60,\n",
        "                     verify=not LOKI_INSECURE, auth=auth, allow_redirects=False)\n",
        "    if r.is_redirect or r.status_code in (301,302,303,307,308):\n",
        "        raise RuntimeError(f\"Auth redirect for tenant={tenant}. Token/headers likely missing.\\n\" + _debug_response(r))\n",
        "    if not r.ok:\n",
        "        raise RuntimeError(f\"Loki query_range failed for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n",
        "        raise RuntimeError(f\"Non-JSON response for tenant={tenant}:\\n\" + _debug_response(r))\n",
        "\n",
        "    payload = r.json()\n",
        "    data = payload.get(\"data\", {}).get(\"result\", [])\n",
        "\n",
        "    def _parse_ns(ts_val):\n",
        "        s = str(ts_val)\n",
        "        try: ns = int(s)\n",
        "        except ValueError: ns = int(float(s))\n",
        "        return pd.to_datetime(ns, unit=\"ns\", utc=True)\n",
        "\n",
        "    rows = []\n",
        "    for series in data:\n",
        "        labels = series.get(\"metric\", {})\n",
        "        for ts, line in series.get(\"values\", []):\n",
        "            rows.append({\"ts\": _parse_ns(ts), \"line\": line, **labels})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Quick tenant sanity\n",
        "for t in [\"application\",\"infrastructure\",\"audit\"]:\n",
        "    try:\n",
        "        info = loki_ping_tenant(t)\n",
        "        print(f\"tenant={t} OK, ~{len(info.get('data', []))} labels\")\n",
        "    except Exception as e:\n",
        "        print(f\"tenant={t} ping failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Pull logs from all tenants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[application] selector: {k8s_namespace_name=~\".+\"}\n",
            "[application] rows: 5000\n",
            "[infrastructure] selector: {k8s_namespace_name=~\".+\"}\n",
            "[infrastructure] rows: 5000\n",
            "[audit] selector: {k8s_node_name=~\".+\"}\n",
            "[audit] rows: 5000\n",
            "df_app   â†’ <class 'pandas.core.frame.DataFrame'> | shape=(5000, 4) | empty=False\n",
            "df_infra â†’ <class 'pandas.core.frame.DataFrame'> | shape=(5000, 4) | empty=False\n",
            "df_audit â†’ <class 'pandas.core.frame.DataFrame'> | shape=(5000, 4) | empty=False\n"
          ]
        }
      ],
      "source": [
        "# === Step 2 â€” SAFE multi-tenant fetch (always returns DataFrames) ===\n",
        "import pandas as pd\n",
        "\n",
        "def tenant_wildcard_selector(tenant: str) -> str:\n",
        "    try:\n",
        "        labels = set(loki_labels_tenant(tenant) or [])\n",
        "    except Exception:\n",
        "        labels = set()\n",
        "    prefs = [\n",
        "        \"k8s_namespace_name\",\"kubernetes_namespace_name\",\"namespace\",\n",
        "        \"k8s_pod_name\",\"kubernetes_pod_name\",\"pod\",\n",
        "        \"k8s_node_name\",\"kubernetes_host\",\"node\",\n",
        "        \"job\",\"filename\",\"log_type\"\n",
        "    ]\n",
        "    for key in prefs:\n",
        "        if key in labels:\n",
        "            return f'{{{key}=~\".+\"}}'\n",
        "    if labels:\n",
        "        key = sorted(labels)[0]\n",
        "        return f'{{{key}=~\".+\"}}'\n",
        "    return '{job=~\".+\"}'\n",
        "\n",
        "def safe_fetch(tenant: str, start, end, step=\"60s\", limit=5000):\n",
        "    try:\n",
        "        _ = loki_ping_tenant(tenant)  # auth/scope sanity\n",
        "        sel = tenant_wildcard_selector(tenant)\n",
        "        print(f\"[{tenant}] selector:\", sel)\n",
        "        df = loki_query_range_tenant(tenant, sel, start, end, step=step, limit=limit)\n",
        "        # Guarantee a DataFrame with expected minimal columns\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            print(f\"[{tenant}] unexpected return type: {type(df)} â†’ coercing to empty DataFrame\")\n",
        "            df = pd.DataFrame(columns=[\"ts\",\"line\"])\n",
        "        if \"ts\" not in df.columns:\n",
        "            df[\"ts\"] = pd.NaT\n",
        "        if \"line\" not in df.columns:\n",
        "            df[\"line\"] = \"\"\n",
        "        print(f\"[{tenant}] rows: {len(df)}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"[{tenant}] fetch failed â†’ {e}\")\n",
        "        return pd.DataFrame(columns=[\"ts\",\"line\"])\n",
        "\n",
        "# Wider window while debugging\n",
        "END   = pd.Timestamp.utcnow()\n",
        "START = END - pd.Timedelta(\"6h\")\n",
        "\n",
        "df_app   = safe_fetch(\"application\",    START, END)\n",
        "df_infra = safe_fetch(\"infrastructure\", START, END)\n",
        "df_audit = safe_fetch(\"audit\",          START, END)\n",
        "\n",
        "def _shape(df):\n",
        "    return f\"{type(df)} | shape={getattr(df,'shape',None)} | empty={getattr(df,'empty',None)}\"\n",
        "print(\"df_app   â†’\", _shape(df_app))\n",
        "print(\"df_infra â†’\", _shape(df_infra))\n",
        "print(\"df_audit â†’\", _shape(df_audit))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) JSONâ€‘first projector (namespace/pod/node from labels or JSON body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Replace your existing projector with this robust version ---\n",
        "import json, re\n",
        "import pandas as pd\n",
        "\n",
        "def _maybe_json(s: str):\n",
        "    if not isinstance(s, str): \n",
        "        return None\n",
        "    s = s.strip()\n",
        "    if not s or s[0] not in \"{[\": \n",
        "        return None\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _get_any(obj, keys):\n",
        "    for k in keys:\n",
        "        cur = obj\n",
        "        try:\n",
        "            for part in k.split(\".\"):\n",
        "                if isinstance(cur, dict) and part in cur:\n",
        "                    cur = cur[part]\n",
        "                else:\n",
        "                    raise KeyError\n",
        "            return cur\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def _normalize_level(obj, line: str):\n",
        "    if isinstance(obj, dict):\n",
        "        v = _get_any(obj, [\"level\",\"severity\",\"loglevel\",\"lvl\",\"logger_level\"])\n",
        "        if v is not None: return str(v).lower()\n",
        "    s = (line or \"\").lower()\n",
        "    if any(w in s for w in [\"error\",\"exception\",\"fail\",\"backoff\",\"oomkilled\",\"notready\"]): return \"error\"\n",
        "    if \"warn\" in s or \"throttle\" in s: return \"warn\"\n",
        "    return \"info\"\n",
        "\n",
        "def _extract_code(obj, line: str):\n",
        "    if isinstance(obj, dict):\n",
        "        v = _get_any(obj, [\"status\",\"status_code\",\"code\",\"http.status\",\"response.status\"])\n",
        "        try:\n",
        "            if v is not None: return int(v)\n",
        "        except Exception:\n",
        "            pass\n",
        "    m = re.search(r\"\\s(1\\d{2}|2\\d{2}|3\\d{2}|4\\d{2}|5\\d{2})\\s\", \" \" + (line or \"\") + \" \")\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _extract_route(obj, line: str):\n",
        "    if isinstance(obj, dict):\n",
        "        v = _get_any(obj, [\"path\",\"route\",\"url\",\"request_path\",\"http.path\",\"request.url\",\"endpoint\"])\n",
        "        if isinstance(v, str): return v.split(\"?\")[0]\n",
        "    m = re.search(r\"\\s(?:GET|POST|PUT|PATCH|DELETE)\\s+(\\S+)\", \" \" + (line or \"\") + \" \")\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def _to_text(v):\n",
        "    if v is None: return \"\"\n",
        "    if isinstance(v, str): return v\n",
        "    try:\n",
        "        return json.dumps(v, default=str)\n",
        "    except Exception:\n",
        "        return str(v)\n",
        "\n",
        "def project_unified_stronger(df: pd.DataFrame, source_guess: str) -> pd.DataFrame:\n",
        "    # 1) Coerce 'line' to text safely (fixes .str accessor errors)\n",
        "    if \"line\" not in df.columns:\n",
        "        line_text = pd.Series([\"\"] * len(df), index=df.index)\n",
        "    else:\n",
        "        line_text = df[\"line\"].map(_to_text).astype(\"string\")  # ensure string dtype\n",
        "\n",
        "    # 2) Prefer k8s label columns; else pull from JSON body\n",
        "    ns_series   = df.get(\"k8s_namespace_name\") or df.get(\"kubernetes_namespace_name\") or df.get(\"namespace\")\n",
        "    pod_series  = df.get(\"k8s_pod_name\")       or df.get(\"kubernetes_pod_name\")       or df.get(\"pod\")\n",
        "    node_series = df.get(\"k8s_node_name\")      or df.get(\"kubernetes_host\")           or df.get(\"node\")\n",
        "\n",
        "    # Parse JSON after coercion\n",
        "    objs = line_text.map(_maybe_json)\n",
        "\n",
        "    def _label_or_json(series_or_none, json_keys):\n",
        "        if series_or_none is not None:\n",
        "            return series_or_none\n",
        "        vals = []\n",
        "        for o in objs:\n",
        "            v = _get_any(o, json_keys) if isinstance(o, dict) else None\n",
        "            vals.append(v)\n",
        "        return pd.Series(vals, index=line_text.index)\n",
        "\n",
        "    namespace = _label_or_json(ns_series,   [\"kubernetes.namespace_name\",\"k8s.namespace.name\",\"k8s.ns\",\"namespace\"])\n",
        "    pod       = _label_or_json(pod_series,  [\"kubernetes.pod_name\",\"k8s.pod.name\",\"pod\"])\n",
        "    node      = _label_or_json(node_series, [\"kubernetes.host\",\"kubernetes.node_name\",\"k8s.node.name\",\"node\"])\n",
        "\n",
        "    # Features\n",
        "    level = [_normalize_level(o, ln) for o, ln in zip(objs, line_text)]\n",
        "    code  = [_extract_code(o, ln)    for o, ln in zip(objs, line_text)]\n",
        "    route = [_extract_route(o, ln)   for o, ln in zip(objs, line_text)]\n",
        "\n",
        "    container_restart = line_text.str.contains(r\"\\bRestarted container\\b\", case=False, na=False).astype(int)\n",
        "    rollout_hit = line_text.str.contains(\n",
        "        r\"Scaled up replica set|deployment (created|updated|rolled out)|\\brollout\\b\",\n",
        "        case=False, na=False, regex=True\n",
        "    ).astype(float)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"ts\": df.get(\"ts\", pd.NaT),\n",
        "        \"source\": source_guess,\n",
        "        \"namespace\": namespace,\n",
        "        \"pod\": pod,\n",
        "        \"node\": node,\n",
        "        \"level\": level,\n",
        "        \"verb\": None,\n",
        "        \"code\": code,\n",
        "        \"route\": route,\n",
        "        \"msg\": line_text.str.slice(0, 400),\n",
        "        \"container_restart\": container_restart,\n",
        "        \"rollout_in_window\": rollout_hit,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Concat, write `latest.parquet`, and show quick stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3a270f64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_app   â†’ (5000, 4), empty=False\n",
            "df_infra â†’ (5000, 4), empty=False\n",
            "df_audit â†’ (5000, 4), empty=False\n",
            "\n",
            "app columns: ['ts', 'line', 'ts', 'line']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-10 13:10:23.900629823+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:23.900629823Z\"...</td>\n",
              "      <td>2025-09-10 13:10:23.900629823+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:23.900629823Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-10 13:10:24.315266063+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:24.315266063Z\"...</td>\n",
              "      <td>2025-09-10 13:10:24.315266063+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:24.315266063Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-10 13:10:24.315570277+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:24.315570277Z\"...</td>\n",
              "      <td>2025-09-10 13:10:24.315570277+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:24.315570277Z\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:23.900629823+00:00   \n",
              "1 2025-09-10 13:10:24.315266063+00:00   \n",
              "2 2025-09-10 13:10:24.315570277+00:00   \n",
              "\n",
              "                                                line  \\\n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:23.900629823Z\"...   \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:24.315266063Z\"...   \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:24.315570277Z\"...   \n",
              "\n",
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:23.900629823+00:00   \n",
              "1 2025-09-10 13:10:24.315266063+00:00   \n",
              "2 2025-09-10 13:10:24.315570277+00:00   \n",
              "\n",
              "                                                line  \n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:23.900629823Z\"...  \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:24.315266063Z\"...  \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:24.315570277Z\"...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "infra columns: ['ts', 'line', 'ts', 'line']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-10 13:10:26.097185665+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:26.097185665Z\"...</td>\n",
              "      <td>2025-09-10 13:10:26.097185665+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:26.097185665Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-10 13:10:12.298212602+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:12.298212602Z\"...</td>\n",
              "      <td>2025-09-10 13:10:12.298212602+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:12.298212602Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-10 13:10:14.845378415+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:14.845378415Z\"...</td>\n",
              "      <td>2025-09-10 13:10:14.845378415+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:14.845378415Z\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:26.097185665+00:00   \n",
              "1 2025-09-10 13:10:12.298212602+00:00   \n",
              "2 2025-09-10 13:10:14.845378415+00:00   \n",
              "\n",
              "                                                line  \\\n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:26.097185665Z\"...   \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:12.298212602Z\"...   \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:14.845378415Z\"...   \n",
              "\n",
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:26.097185665+00:00   \n",
              "1 2025-09-10 13:10:12.298212602+00:00   \n",
              "2 2025-09-10 13:10:14.845378415+00:00   \n",
              "\n",
              "                                                line  \n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:26.097185665Z\"...  \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:12.298212602Z\"...  \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:14.845378415Z\"...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "audit columns: ['ts', 'line', 'ts', 'line']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "      <th>ts</th>\n",
              "      <th>line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-10 13:10:10.411414683+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.411414683Z\"...</td>\n",
              "      <td>2025-09-10 13:10:10.411414683+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.411414683Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-10 13:10:10.428389732+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.428389732Z\"...</td>\n",
              "      <td>2025-09-10 13:10:10.428389732+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.428389732Z\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-10 13:10:10.428414040+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.428414040Z\"...</td>\n",
              "      <td>2025-09-10 13:10:10.428414040+00:00</td>\n",
              "      <td>{\"@timestamp\":\"2025-09-10T13:10:10.428414040Z\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:10.411414683+00:00   \n",
              "1 2025-09-10 13:10:10.428389732+00:00   \n",
              "2 2025-09-10 13:10:10.428414040+00:00   \n",
              "\n",
              "                                                line  \\\n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:10.411414683Z\"...   \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:10.428389732Z\"...   \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:10.428414040Z\"...   \n",
              "\n",
              "                                   ts  \\\n",
              "0 2025-09-10 13:10:10.411414683+00:00   \n",
              "1 2025-09-10 13:10:10.428389732+00:00   \n",
              "2 2025-09-10 13:10:10.428414040+00:00   \n",
              "\n",
              "                                                line  \n",
              "0  {\"@timestamp\":\"2025-09-10T13:10:10.411414683Z\"...  \n",
              "1  {\"@timestamp\":\"2025-09-10T13:10:10.428389732Z\"...  \n",
              "2  {\"@timestamp\":\"2025-09-10T13:10:10.428414040Z\"...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "def _shape(df):\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        return f\"{type(df)}\"\n",
        "    return f\"{df.shape}, empty={df.empty}\"\n",
        "\n",
        "print(\"df_app   â†’\", _shape(df_app))\n",
        "print(\"df_infra â†’\", _shape(df_infra))\n",
        "print(\"df_audit â†’\", _shape(df_audit))\n",
        "\n",
        "# If any have rows, show their columns so projector can map labels\n",
        "for name, df in [(\"app\", df_app), (\"infra\", df_infra), (\"audit\", df_audit)]:\n",
        "    if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "        print(f\"\\n{name} columns:\", list(df.columns))\n",
        "        display(df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sizes -> app/infra/audit: 5000 5000 5000\n",
            "Empty flags -> False False False\n",
            "Using projector: project_unified_stronger\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Length of values (2) does not match length of index (5000)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m parts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df_app, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_app\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 10\u001b[0m     parts\u001b[38;5;241m.\u001b[39mappend(project_unified_stronger(df_app, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df_infra, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_infra\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     12\u001b[0m     parts\u001b[38;5;241m.\u001b[39mappend(project_unified_stronger(df_infra, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfra\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "Cell \u001b[1;32mIn[6], line 88\u001b[0m, in \u001b[0;36mproject_unified_stronger\u001b[1;34m(df, source_guess)\u001b[0m\n\u001b[0;32m     85\u001b[0m         vals\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(vals, index\u001b[38;5;241m=\u001b[39mline_text\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 88\u001b[0m namespace \u001b[38;5;241m=\u001b[39m _label_or_json(ns_series,   [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkubernetes.namespace_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk8s.namespace.name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk8s.ns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     89\u001b[0m pod       \u001b[38;5;241m=\u001b[39m _label_or_json(pod_series,  [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkubernetes.pod_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk8s.pod.name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpod\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m node      \u001b[38;5;241m=\u001b[39m _label_or_json(node_series, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkubernetes.host\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkubernetes.node_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk8s.node.name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[1;32mIn[6], line 86\u001b[0m, in \u001b[0;36mproject_unified_stronger.<locals>._label_or_json\u001b[1;34m(series_or_none, json_keys)\u001b[0m\n\u001b[0;32m     84\u001b[0m     v \u001b[38;5;241m=\u001b[39m _get_any(o, json_keys) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     vals\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(vals, index\u001b[38;5;241m=\u001b[39mline_text\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:575\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    573\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 575\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(data, index)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Length of values (2) does not match length of index (5000)"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Sizes -> app/infra/audit:\", len(df_app), len(df_infra), len(df_audit))\n",
        "print(\"Empty flags ->\", df_app.empty, df_infra.empty, df_audit.empty)\n",
        "print(\"Using projector:\", project_unified_stronger.__name__)\n",
        "\n",
        "parts = []\n",
        "if isinstance(df_app, pd.DataFrame) and not df_app.empty:\n",
        "    parts.append(project_unified_stronger(df_app, \"app\"))\n",
        "if isinstance(df_infra, pd.DataFrame) and not df_infra.empty:\n",
        "    parts.append(project_unified_stronger(df_infra, \"infra\"))\n",
        "if isinstance(df_audit, pd.DataFrame) and not df_audit.empty:\n",
        "    parts.append(project_unified_stronger(df_audit, \"audit\"))\n",
        "\n",
        "if parts:\n",
        "    unified = pd.concat(parts, ignore_index=True)\n",
        "else:\n",
        "    print(\"No rows from any tenant in this window. Creating an empty unified frame.\")\n",
        "    unified = pd.DataFrame(columns=[\n",
        "        \"ts\",\"source\",\"namespace\",\"pod\",\"node\",\"level\",\"verb\",\"code\",\"route\",\"msg\",\n",
        "        \"container_restart\",\"rollout_in_window\"\n",
        "    ])\n",
        "\n",
        "# Dtypes & cleanup\n",
        "if not unified.empty:\n",
        "    unified[\"ts\"] = pd.to_datetime(unified[\"ts\"], utc=True, errors=\"coerce\")\n",
        "    unified = unified.dropna(subset=[\"ts\"]).sort_values(\"ts\").reset_index(drop=True)\n",
        "    unified[\"container_restart\"] = pd.to_numeric(unified[\"container_restart\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "    unified[\"code\"] = pd.to_numeric(unified[\"code\"], errors=\"coerce\")\n",
        "else:\n",
        "    # ensure ts exists with correct dtype\n",
        "    unified[\"ts\"] = pd.to_datetime(unified[\"ts\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "UNIFIED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "unified_path = UNIFIED_DIR / \"latest.parquet\"\n",
        "csv_path     = UNIFIED_DIR / \"latest.csv\"\n",
        "\n",
        "# Try Parquet; fall back to CSV if engine missing\n",
        "try:\n",
        "    import pyarrow  # noqa: F401\n",
        "    unified.to_parquet(unified_path, index=False)\n",
        "    wrote = f\"parquet â†’ {unified_path}\"\n",
        "except Exception as e:\n",
        "    print(\"Parquet write failed:\", e)\n",
        "    print(\"Falling back to CSV.\")\n",
        "    unified.to_csv(csv_path, index=False)\n",
        "    wrote = f\"csv â†’ {csv_path}\\nHint: install Parquet engine with:  %pip install pyarrow\"\n",
        "\n",
        "print(\"Unified rows:\", len(unified))\n",
        "print(\"Wrote:\", wrote)\n",
        "print(\"Nulls by column:\\n\", unified.isna().mean().round(3))\n",
        "if not unified.empty:\n",
        "    print(\"Level distribution:\\n\", unified[\"level\"].value_counts(dropna=False).head(10))\n",
        "    print(\"HTTP status sample:\\n\", unified[\"code\"].dropna().astype(int).value_counts().head(10))\n",
        "display(unified.head(8))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build 10â€‘minute episodes (namespace/pod/node groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c7adbb",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'unified' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m episodes\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Build episodes\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m eps \u001b[38;5;241m=\u001b[39m build_episodes(unified, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Summarize safely\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m eps:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'unified' is not defined"
          ]
        }
      ],
      "source": [
        "# === Step 5 â€” Build 10-minute episodes (safe when no data) ===\n",
        "import pandas as pd\n",
        "\n",
        "def build_episodes(df: pd.DataFrame, window=\"10min\", keys=(\"namespace\",\"pod\",\"node\")):\n",
        "    if df is None or df.empty:\n",
        "        return []\n",
        "    df = df.copy()\n",
        "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"ts\"])\n",
        "    if df.empty:\n",
        "        return []\n",
        "\n",
        "    df.set_index(\"ts\", inplace=True)\n",
        "    episodes = []\n",
        "    for wstart, wdf in df.groupby(pd.Grouper(freq=window)):\n",
        "        if wdf.empty:\n",
        "            continue\n",
        "        wend = wstart + pd.to_timedelta(window)\n",
        "        grp_cols = [k for k in keys if k in wdf.columns]\n",
        "        groups = dict(tuple(wdf.groupby(grp_cols, dropna=False))) if grp_cols else {\"_\": wdf}\n",
        "        for gkey, gdf in groups.items():\n",
        "            total = len(gdf)\n",
        "            errors = (gdf.get(\"level\",\"\").eq(\"error\")).sum()\n",
        "            err_ratio = (errors/total) if total else 0.0\n",
        "            restarts = gdf.get(\"container_restart\", pd.Series([0]*total, index=gdf.index)).sum()\n",
        "            http5xx = (gdf.get(\"code\", pd.Series(dtype=float)) >= 500).sum()\n",
        "            rollout = 1.0 if (gdf.get(\"rollout_in_window\", pd.Series(dtype=float)) > 0).any() else 0.0\n",
        "            entities = {}\n",
        "            for col in [\"namespace\",\"pod\",\"node\"]:\n",
        "                if col in gdf.columns:\n",
        "                    vals = [v for v in gdf[col].astype(str).dropna().unique().tolist() if v and v != \"None\"]\n",
        "                    if vals:\n",
        "                        entities[col] = vals\n",
        "            episodes.append({\n",
        "                \"episode_id\": f\"{int(wstart.value)}::{hash(str(gkey)) & 0xfffffff:07x}\",\n",
        "                \"start\": wstart,\n",
        "                \"end\": wend,\n",
        "                \"entities\": entities,\n",
        "                \"features\": {\n",
        "                    \"count\": float(total),\n",
        "                    \"error_ratio\": float(err_ratio),\n",
        "                    \"restarts\": float(restarts),\n",
        "                    \"http5xx\": float(http5xx),\n",
        "                    \"rollout_in_window\": rollout,\n",
        "                },\n",
        "            })\n",
        "    return episodes\n",
        "\n",
        "# Build episodes\n",
        "eps = build_episodes(unified, window=\"10min\")\n",
        "\n",
        "# Summarize safely\n",
        "if not eps:\n",
        "    print(\"No episodes built. Likely because `unified` is empty or the window has no logs.\")\n",
        "    print(\"Tip: widen the window (e.g., START = END - pd.Timedelta('24h')) and re-fetch;\")\n",
        "    print(\"     or re-run the diagnostics selector step to ensure your queries return rows.\")\n",
        "    epi_dbg = pd.DataFrame(columns=[\"id\",\"count\",\"error_ratio\",\"restarts\",\"http5xx\",\"rollout_in_window\",\n",
        "                                    \"ent_namespace\",\"ent_pod\",\"ent_node\",\"start\",\"end\"])\n",
        "else:\n",
        "    epi_dbg = pd.DataFrame([\n",
        "        {\n",
        "            \"id\": e[\"episode_id\"],\n",
        "            \"count\": e[\"features\"][\"count\"],\n",
        "            \"error_ratio\": e[\"features\"][\"error_ratio\"],\n",
        "            \"restarts\": e[\"features\"][\"restarts\"],\n",
        "            \"http5xx\": e[\"features\"][\"http5xx\"],\n",
        "            \"rollout_in_window\": e[\"features\"][\"rollout_in_window\"],\n",
        "            \"ent_namespace\": \",\".join(e[\"entities\"].get(\"namespace\", [])),\n",
        "            \"ent_pod\": \",\".join(e[\"entities\"].get(\"pod\", [])),\n",
        "            \"ent_node\": \",\".join(e[\"entities\"].get(\"node\", [])),\n",
        "            \"start\": e[\"start\"],\n",
        "            \"end\": e[\"end\"],\n",
        "        }\n",
        "        for e in eps\n",
        "    ]).sort_values([\"start\",\"id\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Episodes:\", len(eps))\n",
        "display(epi_dbg.head(12))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
