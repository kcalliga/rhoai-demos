{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83d\udd0e Loki (multi-tenant) \u2192 Parquet \u2192 Episodes\n\nThis notebook pulls logs from **application**, **infrastructure**, and **audit** tenants on an OpenShift Loki gateway,\nnormalizes them with a JSON-first projector, writes `data/unified_logs/latest.parquet` & CSV, and builds 10\u2011minute episodes.\nIt includes robust auth/SSL handling, nanosecond timestamp parsing, and tenant-specific helpers.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# If needed, install deps (uncomment once)\n# %pip install --quiet pandas numpy requests pyarrow\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pathlib import Path\nimport os, pandas as pd, numpy as np\n\n# --- Storage locations\nDATA_DIR = Path(\"data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\nUNIFIED_DIR = DATA_DIR / \"unified_logs\"; UNIFIED_DIR.mkdir(exist_ok=True, parents=True)\nINCIDENTS_DIR = Path(\"incidents\"); INCIDENTS_DIR.mkdir(exist_ok=True, parents=True)\nRULES_DIR = Path(\"rules\"); RULES_DIR.mkdir(exist_ok=True, parents=True)\n\n# --- Time window (adjust as needed)\nEND   = pd.Timestamp.utcnow()\nSTART = END - pd.Timedelta(\"90min\")\nprint(\"Window:\", START, \"\u2192\", END)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Loki helpers (tenant\u2011aware + token + SSL toggle)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Cell 1.1 \u2014 config + session + diagnostics\nimport pandas as pd, requests, urllib3\n\n# ---- Config (set env vars or edit here) ----\nLOKI_BASE       = os.environ.get(\"LOKI_BASE\", \"https://logging-loki-openshift-logging.apps.example.com\")\nLOKI_TOKEN      = os.environ.get(\"LOKI_TOKEN\")                   # e.g. export LOKI_TOKEN=\"$(oc whoami -t)\"\nLOKI_INSECURE   = os.environ.get(\"LOKI_INSECURE\", \"true\").lower() in (\"1\",\"true\",\"yes\")\nLOKI_ORG_ID     = os.environ.get(\"LOKI_ORG_ID\")                  # some gateways require X-Scope-OrgID\nLOKI_BASIC_USER = os.environ.get(\"LOKI_BASIC_USER\")\nLOKI_BASIC_PASS = os.environ.get(\"LOKI_BASIC_PASS\")\n\nif LOKI_INSECURE:\n    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n_session = requests.Session()\n_default_headers = {\"Accept\": \"application/json\"}\nif LOKI_TOKEN:\n    _default_headers[\"Authorization\"] = f\"Bearer {LOKI_TOKEN}\"\nif LOKI_ORG_ID:\n    _default_headers[\"X-Scope-OrgID\"] = LOKI_ORG_ID\n\ndef _debug_response(resp):\n    ct = resp.headers.get(\"Content-Type\", \"\")\n    preview = (resp.text or \"\")[:500]\n    return f\"HTTP {resp.status_code} CT={ct} URL={resp.url}\\nBody (first 500):\\n{preview}\"\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Cell 1.2 \u2014 tenant ping / labels / query_range (nanosecond safe)\ndef loki_ping_tenant(tenant: str):\n    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/labels\"\n    r = _session.get(url, headers=_default_headers, timeout=30, verify=not LOKI_INSECURE, allow_redirects=False)\n    if not r.ok or \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n        raise RuntimeError(f\"Ping failed for tenant={tenant}:\\n\" + _debug_response(r))\n    return r.json()\n\ndef loki_labels_tenant(tenant: str):\n    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/labels\"\n    r = _session.get(url, headers=_default_headers, timeout=30, verify=not LOKI_INSECURE, allow_redirects=False)\n    r.raise_for_status()\n    return r.json().get(\"data\", [])\n\ndef loki_query_range_tenant(tenant: str, expr, start_ts, end_ts, step='15s', limit=5000, direction='forward'):\n    url = f\"{LOKI_BASE.rstrip('/')}/api/logs/v1/{tenant}/loki/api/v1/query_range\"\n    params = {\n        \"query\": expr,\n        \"start\": int(pd.Timestamp(start_ts).value),  # ns\n        \"end\": int(pd.Timestamp(end_ts).value),\n        \"step\": step,\n        \"limit\": str(limit),\n        \"direction\": direction,\n    }\n    auth = (LOKI_BASIC_USER, LOKI_BASIC_PASS) if (LOKI_BASIC_USER and LOKI_BASIC_PASS) else None\n    r = _session.get(url, params=params, headers=_default_headers, timeout=60,\n                     verify=not LOKI_INSECURE, auth=auth, allow_redirects=False)\n    if r.is_redirect or r.status_code in (301,302,303,307,308):\n        raise RuntimeError(f\"Auth redirect for tenant={tenant}. Token/headers likely missing.\\n\" + _debug_response(r))\n    if not r.ok:\n        raise RuntimeError(f\"Loki query_range failed for tenant={tenant}:\\n\" + _debug_response(r))\n    if \"application/json\" not in r.headers.get(\"Content-Type\",\"\").lower():\n        raise RuntimeError(f\"Non-JSON response for tenant={tenant}:\\n\" + _debug_response(r))\n\n    payload = r.json()\n    data = payload.get(\"data\", {}).get(\"result\", [])\n\n    def _parse_ns(ts_val):\n        s = str(ts_val)\n        try: ns = int(s)\n        except ValueError: ns = int(float(s))\n        return pd.to_datetime(ns, unit=\"ns\", utc=True)\n\n    rows = []\n    for series in data:\n        labels = series.get(\"metric\", {})\n        for ts, line in series.get(\"values\", []):\n            rows.append({\"ts\": _parse_ns(ts), \"line\": line, **labels})\n    return pd.DataFrame(rows)\n\n# Quick tenant sanity\nfor t in [\"application\",\"infrastructure\",\"audit\"]:\n    try:\n        info = loki_ping_tenant(t)\n        print(f\"tenant={t} OK, ~{len(info.get('data', []))} labels\")\n    except Exception as e:\n        print(f\"tenant={t} ping failed:\", e)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Pull logs from all tenants"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Start wide ({}). Narrow with label filters once you confirm data exists.\nLOGQL_ANY = r'{}'\nEND   = pd.Timestamp.utcnow()\nSTART = END - pd.Timedelta(\"90min\")\n\ndf_app   = loki_query_range_tenant(\"application\",   LOGQL_ANY, START, END, step=\"30s\", limit=5000)\ndf_infra = loki_query_range_tenant(\"infrastructure\", LOGQL_ANY, START, END, step=\"30s\", limit=5000)\ndf_audit = loki_query_range_tenant(\"audit\",          LOGQL_ANY, START, END, step=\"30s\", limit=5000)\n\nprint(\"Rows -> app/infra/audit:\", len(df_app), len(df_infra), len(df_audit))\n# Uncomment to preview columns:\n# print(\"app columns:\", list(df_app.columns))\n# print(\"infra columns:\", list(df_infra.columns))\n# print(\"audit columns:\", list(df_audit.columns))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) JSON\u2011first projector (namespace/pod/node from labels or JSON body)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import json, re\n\ndef _maybe_json(s: str):\n    if not isinstance(s, str): return None\n    s = s.strip()\n    if not s or s[0] not in \"{[\": return None\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef _get_any(obj, keys):\n    for k in keys:\n        cur = obj\n        try:\n            for part in k.split(\".\"):\n                if isinstance(cur, dict) and part in cur:\n                    cur = cur[part]\n                else:\n                    raise KeyError\n            return cur\n        except Exception:\n            continue\n    return None\n\ndef _normalize_level(obj, line: str):\n    if isinstance(obj, dict):\n        v = _get_any(obj, [\"level\",\"severity\",\"loglevel\",\"lvl\",\"logger_level\"])\n        if v is not None: return str(v).lower()\n    s = (line or \"\").lower()\n    if any(w in s for w in [\"error\",\"exception\",\"fail\",\"backoff\",\"oomkilled\",\"notready\"]): return \"error\"\n    if \"warn\" in s or \"throttle\" in s: return \"warn\"\n    return \"info\"\n\ndef _extract_code(obj, line: str):\n    if isinstance(obj, dict):\n        v = _get_any(obj, [\"status\",\"status_code\",\"code\",\"http.status\",\"response.status\"])\n        try:\n            if v is not None: return int(v)\n        except Exception:\n            pass\n    import re\n    m = re.search(r\"\\s(1\\d{2}|2\\d{2}|3\\d{2}|4\\d{2}|5\\d{2})\\s\", \" \" + (line or \"\") + \" \")\n    return int(m.group(1)) if m else None\n\ndef _extract_route(obj, line: str):\n    if isinstance(obj, dict):\n        v = _get_any(obj, [\"path\",\"route\",\"url\",\"request_path\",\"http.path\",\"request.url\",\"endpoint\"])\n        if isinstance(v, str): return v.split(\"?\")[0]\n    import re\n    m = re.search(r\"\\s(?:GET|POST|PUT|PATCH|DELETE)\\s+(\\S+)\", \" \" + (line or \"\") + \" \")\n    return m.group(1) if m else None\n\ndef _label_or_json(series_or_none, objs, json_keys):\n    if series_or_none is not None:\n        return series_or_none\n    vals = []\n    for o in objs:\n        v = _get_any(o, json_keys) if isinstance(o, dict) else None\n        vals.append(v)\n    import pandas as pd\n    return pd.Series(vals)\n\ndef project_unified_stronger(df: pd.DataFrame, source_guess: str) -> pd.DataFrame:\n    objs = df[\"line\"].map(_maybe_json)\n\n    # Prefer k8s_* label columns; else read from JSON\n    ns_series   = df.get(\"k8s_namespace_name\") or df.get(\"kubernetes_namespace_name\") or df.get(\"namespace\")\n    pod_series  = df.get(\"k8s_pod_name\")       or df.get(\"kubernetes_pod_name\")       or df.get(\"pod\")\n    node_series = df.get(\"k8s_node_name\")      or df.get(\"kubernetes_host\")           or df.get(\"node\")\n\n    namespace = _label_or_json(ns_series,   objs, [\"kubernetes.namespace_name\",\"k8s.namespace.name\",\"k8s.ns\",\"namespace\"])\n    pod       = _label_or_json(pod_series,  objs, [\"kubernetes.pod_name\",\"k8s.pod.name\",\"pod\"])\n    node      = _label_or_json(node_series, objs, [\"kubernetes.host\",\"kubernetes.node_name\",\"k8s.node.name\",\"node\"])\n\n    level = [_normalize_level(o, ln) for o, ln in zip(objs, df[\"line\"])]\n    code  = [_extract_code(o, ln)    for o, ln in zip(objs, df[\"line\"])]\n    route = [_extract_route(o, ln)   for o, ln in zip(objs, df[\"line\"])]\n\n    container_restart = df[\"line\"].str.contains(r\"\\bRestarted container\\b\", case=False, na=False).astype(int)\n    rollout_hit = df[\"line\"].str.contains(\n        r\"Scaled up replica set|deployment (created|updated|rolled out)|\\brollout\\b\",\n        case=False, na=False, regex=True\n    ).astype(float)\n\n    return pd.DataFrame({\n        \"ts\": df[\"ts\"],\n        \"source\": source_guess,\n        \"namespace\": namespace,\n        \"pod\": pod,\n        \"node\": node,\n        \"level\": level,\n        \"verb\": None,\n        \"code\": code,\n        \"route\": route,\n        \"msg\": df[\"line\"].astype(str).str.slice(0, 400),\n        \"container_restart\": container_restart,\n        \"rollout_in_window\": rollout_hit,\n    })\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Concat, write `latest.parquet`, and show quick stats"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pathlib import Path\n\nprint(\"Sizes -> app/infra/audit:\", len(df_app), len(df_infra), len(df_audit))\n\nparts = []\nif not df_app.empty:   parts.append(project_unified_stronger(df_app,   \"app\"))\nif not df_infra.empty: parts.append(project_unified_stronger(df_infra, \"infra\"))\nif not df_audit.empty: parts.append(project_unified_stronger(df_audit, \"audit\"))\n\nif parts:\n    unified = pd.concat(parts, ignore_index=True)\nelse:\n    unified = pd.DataFrame(columns=[\n        \"ts\",\"source\",\"namespace\",\"pod\",\"node\",\"level\",\"verb\",\"code\",\"route\",\"msg\",\n        \"container_restart\",\"rollout_in_window\"\n    ])\n\n# Dtypes & cleanup\nunified[\"ts\"] = pd.to_datetime(unified[\"ts\"], utc=True, errors=\"coerce\")\nunified = unified.dropna(subset=[\"ts\"]).sort_values(\"ts\").reset_index(drop=True)\nunified[\"container_restart\"] = pd.to_numeric(unified[\"container_restart\"], errors=\"coerce\").fillna(0).astype(\"int64\")\nunified[\"code\"] = pd.to_numeric(unified[\"code\"], errors=\"coerce\")\n\n# Write\nunified_path = UNIFIED_DIR / \"latest.parquet\"\nunified.to_parquet(unified_path, index=False)\nunified_csv = UNIFIED_DIR / \"latest.csv\"\nunified.to_csv(unified_csv, index=False)\n\nprint(\"Unified rows:\", len(unified))\nprint(\"Nulls by column:\\n\", unified.isna().mean().round(3))\nprint(\"Level distribution:\\n\", unified[\"level\"].value_counts(dropna=False).head(10))\nprint(\"HTTP status sample:\\n\", unified[\"code\"].dropna().astype(int).value_counts().head(10))\nunified.head(8)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Build 10\u2011minute episodes (namespace/pod/node groups)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def build_episodes(df: pd.DataFrame, window=\"10min\", keys=(\"namespace\",\"pod\",\"node\")):\n    df = df.copy()\n    df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True)\n    df.set_index(\"ts\", inplace=True)\n    episodes = []\n    for wstart, wdf in df.groupby(pd.Grouper(freq=window)):\n        if wdf.empty: continue\n        wend = wstart + pd.to_timedelta(window)\n        grp_cols = [k for k in keys if k in wdf.columns]\n        groups = dict(tuple(wdf.groupby(grp_cols, dropna=False))) if grp_cols else {\"_\": wdf}\n        for gkey, gdf in groups.items():\n            total = len(gdf)\n            errors = (gdf[\"level\"]==\"error\").sum()\n            err_ratio = (errors/total) if total else 0.0\n            restarts = gdf.get(\"container_restart\", pd.Series([0]*total, index=gdf.index)).sum()\n            http5xx = (gdf.get(\"code\", pd.Series(dtype=float))>=500).sum()\n            rollout = 1.0 if (gdf.get(\"rollout_in_window\", pd.Series(dtype=float))>0).any() else 0.0\n            entities = {}\n            for col in [\"namespace\",\"pod\",\"node\"]:\n                vals = [v for v in gdf[col].astype(str).dropna().unique().tolist() if v and v!=\"None\"]\n                if vals: entities[col] = vals\n            episodes.append({\n                \"episode_id\": f\"{int(wstart.value)}::{hash(str(gkey)) & 0xfffffff:07x}\",\n                \"start\": wstart, \"end\": wend,\n                \"entities\": entities,\n                \"features\": {\"count\": float(total), \"error_ratio\": float(err_ratio), \"restarts\": float(restarts), \"http5xx\": float(http5xx), \"rollout_in_window\": rollout},\n            })\n    return episodes\n\neps = build_episodes(unified, window=\"10min\")\nprint(\"Episodes:\", len(eps))\nimport pandas as pd\nepi_dbg = pd.DataFrame([\n    {\"id\": e[\"episode_id\"], **e[\"features\"], \n     \"ent_namespace\": \",\".join(e[\"entities\"].get(\"namespace\", [])),\n     \"ent_pod\": \",\".join(e[\"entities\"].get(\"pod\", [])),\n     \"ent_node\": \",\".join(e[\"entities\"].get(\"node\", [])),}\n    for e in eps\n]).sort_values(\"id\")\nepi_dbg.head(12)\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}